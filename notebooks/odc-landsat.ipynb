{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Landsat Collection 2\n",
    "\n",
    "This notebook explores the Landsat Collection 2 data on AWS:\n",
    "\n",
    "- [Landsat Collection 2 STAC API](https://landsatlook.usgs.gov/stac-server), a catalog of Landsat data\n",
    "- [pystac-client](https://pystac-client.readthedocs.io/) for searching and access data\n",
    "- [XArray](http://xarray.pydata.org/en/stable/), [pandas](https://pandas.pydata.org/) and [geopandas](https://geopandas.org/) for manipulating data\n",
    "- [Dask](https://dask.org/) for performing parallel, distributed computing\n",
    "- [Coiled.io](https://coiled.io/), a service for hosting Dask clusters\n",
    "- [hvplot](https://hvplot.holoviz.org/) for visualization\n",
    "\n",
    "Shown will be how find data for an area of interest, explore the resulting metadata, perform calculations, and visualize the results.\n",
    "\n",
    "Created by [Element 84](http://element84.com/)"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# initial imports and reusable functions\n",
    "\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "\n",
    "from copy import deepcopy\n",
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import pandas as pd\n",
    "import pystac\n",
    "from shapely.geometry import shape\n",
    "\n",
    "# create a function for later reuse\n",
    "def plot_polygons(data, *args, **kwargs):\n",
    "    return data.hvplot.paths(*args, geo=True, tiles='OSM', xaxis=None, yaxis=None,\n",
    "                             frame_width=600, frame_height=600,\n",
    "                             line_width=3, **kwargs)\n",
    "\n",
    "# convert a list of STAC Items into a GeoDataFrame\n",
    "def items_to_geodataframe(items):\n",
    "    _items = []\n",
    "    for i in items:\n",
    "        _i = deepcopy(i)\n",
    "        _i['geometry'] = shape(_i['geometry'])\n",
    "        _items.append(_i)\n",
    "    gdf = gpd.GeoDataFrame(pd.json_normalize(_items))\n",
    "    for field in ['properties.datetime', 'properties.created', 'properties.updated']:\n",
    "        if field in gdf:\n",
    "            gdf[field] = pd.to_datetime(gdf[field])\n",
    "    gdf.set_index('properties.datetime', inplace=True)\n",
    "    return gdf\n",
    "\n",
    "# set pystac_client logger to DEBUG to see API calls\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger('pystac_client')\n",
    "logger.setLevel(logging.INFO)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Search for data\n",
    "\n",
    "Use pystac-client to find data in the Landsat STAC API."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Open the Landsat STAC API\n",
    "\n",
    "from pystac_client import Client\n",
    "URL = 'https://landsatlook.usgs.gov/stac-server'\n",
    "cat = Client.open(URL)\n",
    "print(cat)\n",
    "\n",
    "collections = [(c.id, c.title) for c in cat.get_collections()]\n",
    "pd.set_option(\"display.max_colwidth\", 150)\n",
    "df = pd.DataFrame(collections, columns=['id', 'title'])\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fetch the collection of interest: Landsat Collection 2, Level 2 Surface Reflectance (landsat-c2l2-sr) and print the assets that are available."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "collection_id = 'landsat-c2l2-sr'\n",
    "\n",
    "collection = cat.get_collection(collection_id)\n",
    "pd.DataFrame.from_dict(collection.to_dict()['item_assets'], orient='index')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Change the AOI, search parameters here, and print how many matching scenes there are."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import geopandas as gpd\n",
    "import json\n",
    "\n",
    "aoi = gpd.read_file('../aois/malawi.geojson')\n",
    "geom = json.loads(aoi['geometry'].to_json())['features'][0]['geometry']\n",
    "\n",
    "# limit sets the # of items per page so we can see multiple pages getting fetched\n",
    "search = cat.search(\n",
    "    collections = [collection_id],\n",
    "    intersects = aoi['geometry'][0],\n",
    "    datetime = \"2021-08-01/2021-08-31\",\n",
    "    query = [\"eo:cloud_cover<10\"],\n",
    "    limit = 100\n",
    ")\n",
    "\n",
    "print(f\"{search.matched()} items found\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use GeoPandas to view footprints"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cell below fetches all the STAC Items and updates the URLs to use the provided s3 URLs which can be used for direct access rather than the default https URLs.\n",
    "\n",
    "Then, we create a GeoDataFrame for visualizing the footprints."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get all items as a dictionary\n",
    "items_dict = search.get_all_items_as_dict()['features']\n",
    "\n",
    "# update URLs to use s3\n",
    "for item in items_dict:\n",
    "    for a in item['assets']:\n",
    "        if 'alternate' in item['assets'][a] and 's3' in item['assets'][a]['alternate']:\n",
    "            item['assets'][a]['href'] = item['assets'][a]['alternate']['s3']['href']\n",
    "\n",
    "# Create GeoDataFrame from Items\n",
    "items_gdf = items_to_geodataframe(items_dict)\n",
    "\n",
    "print(f\"{len(items_dict)} items found\")\n",
    "\n",
    "pd.reset_option(\"display.max_colwidth\")\n",
    "items_gdf.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_polygons(aoi) * items_gdf.hvplot.paths(geo=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# OpenDataCube\n",
    "\n",
    "Now we'll turn the set of scenes into a virtual datacube. None of the data will actually be read yet.\n",
    "\n",
    "The configuration string (`cfg`) is for providing additional info not currently available in the STAC Items, but will be in the future."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import yaml\n",
    "\n",
    "cfg = \"\"\"---\n",
    "landat-c2l2-sr:\n",
    "  measurements:\n",
    "    '*':\n",
    "      dtype: uint16\n",
    "      nodata: 0\n",
    "      unit: 'm'\n",
    "\"\"\"\n",
    "cfg = yaml.load(cfg, Loader=yaml.CSafeLoader)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we load as a DataCube. A PySTAC ItemCollection is created from the found STAC Items, and we specify various parameters, such as bands of interest and chunk size."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "from odc.stac import stac_load\n",
    "\n",
    "# Create PySTAC ItemCollection\n",
    "item_collection = pystac.ItemCollection(items_dict)\n",
    "\n",
    "# default to CRS and resolution from first Item\n",
    "from pystac.extensions.projection import ProjectionExtension\n",
    "from pyproj import CRS\n",
    "\n",
    "proj = ProjectionExtension.ext(item_collection[0])\n",
    "output_crs = CRS.from_epsg(proj.epsg)\n",
    "resolution = (proj.transform[4], proj.transform[0])\n",
    "\n",
    "dc = stac_load(item_collection,\n",
    "               bands=['red', 'blue', 'green', 'nir08'],\n",
    "               chunks={\"x\": 2048, \"y\": 2048},\n",
    "               output_crs=output_crs,\n",
    "               resolution=resolution,\n",
    "               groupby='solar_day',\n",
    "               stac_cfg=cfg\n",
    ")\n",
    "dc"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculations\n",
    "\n",
    "The datacube currently contains complete Items, we want to clip these to our geometry of interest. We will then also create an RGB datacube representation, and generate an NDVI datacube."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "import rioxarray\n",
    "\n",
    "dc = dc.rio.clip([geom], crs='epsg:4326')\n",
    "dc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from odc.algo import to_rgba\n",
    "\n",
    "vis = to_rgba(dc, clamp=(1, 20000), bands=['red', 'green', 'blue'])\n",
    "vis"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ndvi = ((dc['nir08'] - dc['red']) / (dc['nir08'] + dc['red'])).clip(0, 1)\n",
    "ndvi.name = 'ndvi'\n",
    "ndvi"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Start Dask Client\n",
    "\n",
    "Start either a local Dask, or use [coiled.io](coiled.io)"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# local Dask\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "import coiled\n",
    "from dask.distributed import Client\n",
    "\n",
    "# start dask cluster on coiled.io\n",
    "cluster = coiled.Cluster(\n",
    "    n_workers=25, software=\"matthewhanson/geo-notebooks-basic\", backend_options={\"region\": \"us-west-2\"},\n",
    "    environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"YES\", \"AWS_REQUEST_PAYER\": \"requester\"}\n",
    ")\n",
    "# re-use existing cluster\n",
    "#cluster = coiled.Cluster(name=\"matthewhanson-e6818fa6-e\")\n",
    "client = Client(cluster)\n",
    "client"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute\n",
    "\n",
    "Now, we kick off our Dask computation by using the Dask persist function, which will keep the data in memory on the cluster for faster access later.\n",
    "\n",
    "The Dask `compute` function is used when we actually want the data, such as displaying it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "from dask.distributed import wait\n",
    "\n",
    "vis = client.persist(vis)\n",
    "_ = wait(vis)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "vis_ = vis.compute()\n",
    "vis_.plot.imshow(col='time', rgb='band', col_wrap=5, robust=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import hvplot.xarray\n",
    "\n",
    "hvplot_kwargs = {\n",
    "    \"frame_width\": 800,\n",
    "    \"xaxis\": None,\n",
    "    \"yaxis\": None,\n",
    "    \"widget_location\": \"bottom\",\n",
    "    \"aspect\": len(vis.x)/len(vis.y)\n",
    "}\n",
    "\n",
    "vis_.hvplot.rgb('x', 'y', bands='band', groupby='time', **hvplot_kwargs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ndvi_ = ndvi.compute()\n",
    "ndvi_.hvplot('x', 'y', groupby='time', **hvplot_kwargs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create an animated GIF of NDVI over time using `geogif` with the fetched results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from geogif import gif\n",
    "\n",
    "#ndvi_ = ndvi_c.transpose('time','x','y').compute()\n",
    "\n",
    "gif(ndvi_c, fps=1, cmap='YlGn')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "ndvi_mean = ndvi.mean(dim=['x', 'y']).compute()\n",
    "ndvi_mean.hvplot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Shutdown cluster\n",
    "\n",
    "Shut down the cluster."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "client.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}